{"cells":[{"cell_type":"markdown","source":["# RL Hands-on introduction\n","\n","- based on the awesome course by huggingface [Deep Reinforcement Learning](https://huggingface.co/learn/deep-rl-course)\n","\n","\n","![Image](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit0/thumbnail.jpg)"],"metadata":{"id":"C-0EvfJaJ6Wz"}},{"cell_type":"markdown","metadata":{"id":"njb_ProuHiOe"},"source":["# Content\n","\n","- Code your first Reinforcement Learning agent from scratch to\n","- Play FrozenLake ❄️\n","- Understand and apply reinforcement learning concepts, specifically Q-learning\n","- Example video\n"]},{"cell_type":"markdown","source":["###🎮 Environments:\n","\n","- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n","\n","###📚 RL-Library:\n","\n","- Python and NumPy\n","- [Gymnasium](https://gymnasium.farama.org/)"],"metadata":{"id":"DPTBOv9HYLZ2"}},{"cell_type":"markdown","source":["# Set up"],"metadata":{"id":"HEtx8Y8MqKfH"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"9XaULfDZDvrC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712751305305,"user_tz":-180,"elapsed":31331,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}},"outputId":"274e491d-1af3-4a82-e973-9993fc2a9446"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1))\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 2)) (2.5.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 3)) (1.25.2)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (0.20.3)\n","Collecting pickle5 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 6))\n","  Downloading pickle5-0.0.11.tar.gz (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pyyaml==6.0 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 7))\n","  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 8)) (2.31.6)\n","Requirement already satisfied: imageio_ffmpeg in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 9)) (0.4.9)\n","Collecting pyglet==1.5.1 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 10))\n","  Downloading pyglet-1.5.1-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 11)) (4.66.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (4.10.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1))\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.13.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2.31.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (24.0)\n","Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 8)) (9.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio_ffmpeg->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 9)) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2024.2.2)\n","Building wheels for collected packages: pickle5\n","  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=255314 sha256=455cea4ac3993d1e3129aeb799c9c52f4ccd7e1155978f4d017d8d54658f76e4\n","  Stored in directory: /root/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\n","Successfully built pickle5\n","Installing collected packages: pyglet, pickle5, farama-notifications, pyyaml, gymnasium\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 6.0.1\n","    Uninstalling PyYAML-6.0.1:\n","      Successfully uninstalled PyYAML-6.0.1\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pickle5-0.0.11 pyglet-1.5.1 pyyaml-6.0\n"]}],"source":["!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"]},{"cell_type":"code","source":["!sudo apt-get update\n","!sudo apt-get install -y python3-opengl\n","!apt install ffmpeg xvfb\n","!pip3 install pyvirtualdisplay"],"metadata":{"id":"n71uTX7qqzz2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712751341103,"user_tz":-180,"elapsed":35801,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}},"outputId":"05704af4-7e31-4659-ef68-2a8131188ba3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [1 InRelease 5,484 B/110\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [1 InRelease 14.2 kB/110\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [1 InRelease 14.2 kB/110\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [805 kB]\n","Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,690 kB]\n","Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,124 kB]\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,081 kB]\n","Get:15 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,969 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,173 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,357 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [61.3 kB]\n","Fetched 11.5 MB in 2s (4,753 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  freeglut3 libglu1-mesa\n","Suggested packages:\n","  libgle3 python3-numpy\n","The following NEW packages will be installed:\n","  freeglut3 libglu1-mesa python3-opengl\n","0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 824 kB of archives.\n","After this operation, 8,092 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-opengl all 3.1.5+dfsg-1 [605 kB]\n","Fetched 824 kB in 1s (759 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 121752 files and directories currently installed.)\n","Preparing to unpack .../freeglut3_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package python3-opengl.\n","Preparing to unpack .../python3-opengl_3.1.5+dfsg-1_all.deb ...\n","Unpacking python3-opengl (3.1.5+dfsg-1) ...\n","Setting up freeglut3:amd64 (2.8.1-6) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up python3-opengl (3.1.5+dfsg-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 7,813 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.10 [28.5 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.10 [863 kB]\n","Fetched 7,813 kB in 1s (5,448 kB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 124836 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.10_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.10_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n"]}]},{"cell_type":"markdown","source":["To make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so you'll need to connect again and run the code starting from here**. Thanks to this trick, **we will be able to run our virtual screen.**"],"metadata":{"id":"K6XC13pTfFiD"}},{"cell_type":"code","source":["import os\n","os.kill(os.getpid(), 9)"],"metadata":{"id":"3kuZbWAkfHdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"],"metadata":{"id":"DaY1N4dBrabi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"VcNvOAQlysBJ","executionInfo":{"status":"ok","timestamp":1712751349608,"user_tz":-180,"elapsed":999,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}}},"outputs":[],"source":["import numpy as np\n","import gymnasium as gym\n","import random\n","import imageio\n","import os\n","import tqdm\n","\n","import pickle5 as pickle\n","from tqdm.notebook import tqdm"]},{"cell_type":"markdown","metadata":{"id":"xp4-bXKIy1mQ"},"source":["We're now ready to code our Q-Learning algorithm 🔥"]},{"cell_type":"markdown","metadata":{"id":"xya49aNJWVvv"},"source":["# Frozen Lake ⛄ (non slippery version)"]},{"cell_type":"markdown","metadata":{"id":"NAvihuHdy9tw"},"source":["## Create and understand [FrozenLake environment ⛄]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n","---\n","\n","💡 A good habit when you start to use an environment is to check its documentation\n","\n","👉 https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n","\n","---\n","\n","We're going to train our Q-Learning agent, to navigate from the\n","- starting state (S)\n","- to the goal state (G)\n","- by walking only on frozen tiles (F)\n","- and avoid holes (H)\n","\n","We can have two sizes of environment:\n","\n","- `map_name=\"4x4\"`: a 4x4 grid version\n","- `map_name=\"8x8\"`: a 8x8 grid version\n","\n","\n","The environment has two modes:\n","\n","- `is_slippery=False`: The agent always moves **in the intended direction** due to the non-slippery nature of the frozen lake (deterministic).\n","- `is_slippery=True`: The agent **may not always move in the intended direction** due to the slippery nature of the frozen lake (stochastic)."]},{"cell_type":"markdown","metadata":{"id":"UaW_LHfS0PY2"},"source":["For now let's keep it simple with the 4x4 map and non-slippery.\n","We add a parameter called `render_mode` that specifies how the environment should be visualised. In our case because we **want to record a video of the environment at the end, we need to set render_mode to rgb_array**.\n","\n","As [explained in the documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) “rgb_array”: Return a single frame representing the current state of the environment. A frame is a np.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"jNxUbPMP0akP","executionInfo":{"status":"ok","timestamp":1712751439882,"user_tz":-180,"elapsed":219,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}}},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n","\n","#desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n","#gym.make('FrozenLake-v1', desc=desc, is_slippery=True)"]},{"cell_type":"markdown","metadata":{"id":"SXbTfdeJ1Xi9"},"source":["### Let's see what the Environment looks like:\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ZNPG0g_UGCfh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712751473825,"user_tz":-180,"elapsed":251,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}},"outputId":"df134aee-85ea-4f38-8cad-bce368077cca"},"outputs":[{"output_type":"stream","name":"stdout","text":["_____OBSERVATION SPACE_____ \n","\n","Observation Space Discrete(16)\n","Sample observation 3\n"]}],"source":["# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n","print(\"_____OBSERVATION SPACE_____ \\n\")\n","print(\"Observation Space\", env.observation_space)\n","print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"]},{"cell_type":"markdown","metadata":{"id":"2MXc15qFE0M9"},"source":["We see with `Observation Space Shape Discrete(16)` that the observation is an integer representing the **agent’s current position as current_row * ncols + current_col (where both the row and col start at 0)**.\n","\n","For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. **For example, the 4x4 map has 16 possible observations.**\n","\n","\n","For instance, this is what state = 0 looks like:\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"We5WqOBGLoSm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712751507506,"user_tz":-180,"elapsed":330,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}},"outputId":"49f1ae19-2a12-4132-dc04-9c6342d70ea6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," _____ACTION SPACE_____ \n","\n","Action Space Shape 4\n","Action Space Sample 0\n"]}],"source":["print(\"\\n _____ACTION SPACE_____ \\n\")\n","print(\"Action Space Shape\", env.action_space.n)\n","print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"]},{"cell_type":"markdown","metadata":{"id":"MyxXwkI2Magx"},"source":["The action space (the set of possible actions the agent can take) is discrete with 4 actions available 🎮:\n","- 0: GO LEFT\n","- 1: GO DOWN\n","- 2: GO RIGHT\n","- 3: GO UP\n","\n","Reward function 💰:\n","- Reach goal: +1\n","- Reach hole: 0\n","- Reach frozen: 0"]},{"cell_type":"markdown","metadata":{"id":"1pFhWblk3Awr"},"source":["## Create and Initialize the Q-table 🗄️\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"HuTKv3th3ohG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712751550177,"user_tz":-180,"elapsed":230,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}},"outputId":"b801f6da-4dc9-45f0-adb8-ae8e6c2fd3a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are  16  possible states\n","There are  4  possible actions\n"]}],"source":["state_space = env.observation_space.n\n","print(\"There are \", state_space, \" possible states\")\n","\n","action_space = env.action_space.n\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"lnrb_nX33fJo","executionInfo":{"status":"ok","timestamp":1712751551903,"user_tz":-180,"elapsed":287,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}}},"outputs":[],"source":["# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n","def initialize_q_table(state_space, action_space):\n","  Qtable = np.zeros((state_space, action_space))\n","  return Qtable"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Y0WlgkVO3Jf9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712751570060,"user_tz":-180,"elapsed":3,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}},"outputId":"579b3f37-db84-440a-a7f3-cb19ebf3a370"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.]])"]},"metadata":{},"execution_count":12}],"source":["Qtable_frozenlake = initialize_q_table(state_space, action_space)\n","Qtable_frozenlake"]},{"cell_type":"markdown","metadata":{"id":"Atll4Z774gri"},"source":["## Define the greedy policy 🤖\n","\n","Remember we have two policies since Q-Learning:\n","\n","- Epsilon-greedy policy (acting policy)\n","- Greedy-policy (updating policy)\n","\n","The greedy policy will also be the final policy we'll have when the Q-learning agent completes training. The greedy policy is used to select an action using the Q-table."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"se2OzWGW5kYJ","executionInfo":{"status":"ok","timestamp":1712751713613,"user_tz":-180,"elapsed":221,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}}},"outputs":[],"source":["def greedy_policy(Qtable, state):\n","  # Exploitation: take the action with the highest state, action value\n","  action = np.argmax(Qtable[state][:])\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"flILKhBU3yZ7"},"source":["##Define the epsilon-greedy policy 🤖\n","\n","Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.\n","\n","The idea with epsilon-greedy:\n","\n","- With *probability 1 - ɛ* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).\n","\n","- With *probability ɛ*: we do **exploration** (trying a random action).\n","\n","As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation.**"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"cYxHuckr4LiG","executionInfo":{"status":"ok","timestamp":1712751711216,"user_tz":-180,"elapsed":217,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}}},"outputs":[],"source":["def epsilon_greedy_policy(Qtable, state, epsilon):\n","  # Randomly generate a number between 0 and 1\n","  random_num = random.uniform(0,1)\n","  # if random_num > greater than epsilon --> exploitation\n","  if random_num > epsilon:\n","    # Take the action with the highest value given a state\n","    # np.argmax can be useful here\n","    action = greedy_policy(Qtable, state)\n","  # else --> exploration\n","  else:\n","    action = env.action_space.sample()\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"hW80DealcRtu"},"source":["## Define the hyperparameters ⚙️\n","\n","The exploration related hyperparamters are some of the most important ones.\n","\n","- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.\n","- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck**, since your agent didn't explore enough of the state space and hence can't solve the problem."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"Y1tWn0tycWZ1","executionInfo":{"status":"ok","timestamp":1712751796841,"user_tz":-180,"elapsed":269,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}}},"outputs":[],"source":["# Training parameters\n","n_training_episodes = 1000  # Total training episodes\n","learning_rate = 0.7          # Learning rate\n","\n","# Evaluation parameters\n","n_eval_episodes = 100        # Total number of test episodes\n","\n","# Environment parameters\n","env_id = \"FrozenLake-v1\"     # Name of the environment\n","max_steps = 99               # Max steps per episode\n","gamma = 0.95                 # Discounting rate\n","eval_seed = []               # The evaluation seed of the environment\n","\n","# Exploration parameters\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.05            # Minimum exploration probability\n","decay_rate = 0.0005            # Exponential decay rate for exploration prob"]},{"cell_type":"markdown","metadata":{"id":"cDb7Tdx8atfL"},"source":["## Create the training loop method\n","\n","The training loop goes like this:\n","\n","```\n","For episode in the total of training episodes:\n","\n","Reduce epsilon (since we need less and less exploration)\n","Reset the environment\n","\n","  For step in max timesteps:    \n","    Choose the action At using epsilon greedy policy\n","    Take the action (a) and observe the outcome state(s') and reward (r)\n","    Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","    If done, finish the episode\n","    Our next state is the new state\n","```"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"IyZaYbUAeolw","executionInfo":{"status":"ok","timestamp":1712751780680,"user_tz":-180,"elapsed":234,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}}},"outputs":[],"source":["def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n","  for episode in tqdm(range(n_training_episodes)):\n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","    # Reset the environment\n","    state, info = env.reset()\n","    step = 0\n","    terminated = False\n","    truncated = False\n","\n","    # repeat\n","    for step in range(max_steps):\n","      # Choose the action At using epsilon greedy policy\n","      action = epsilon_greedy_policy(Qtable, state, epsilon)\n","\n","      # Take action At and observe Rt+1 and St+1\n","      # Take the action (a) and observe the outcome state(s') and reward (r)\n","      new_state, reward, terminated, truncated, info = env.step(action)\n","\n","      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n","\n","      # If terminated or truncated finish the episode\n","      if terminated or truncated:\n","        break\n","\n","      # Our next state is the new state\n","      state = new_state\n","  return Qtable"]},{"cell_type":"markdown","metadata":{"id":"WLwKQ4tUdhGI"},"source":["## Train the Q-Learning agent 🏃"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"DPBxfjJdTCOH","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["ef583bc15f034db18e1c526ee03fda4f","fdf43db8095641c396e5716cbb85abc8","20a90c62314e4a64b2aee2eebc11c0f8","a8df2395c3f146a082b0972e9bd9f41d","6bfccfb688604aba97116c9e8aa26427","c97fc82d4c954340ae3b4c4e0be28a29","5b52b4e6566245d58c280ac45a0c2053","e549a0a77a5d4c809ef53e6a8d9ff896","4a57ca21d1484c7aa907d0e756298a53","c6f2e991c84c476bbc6543e78b6ddaf0","6a47fececcbc4d7ca39fdc7d7a4c5ee6"]},"executionInfo":{"status":"ok","timestamp":1712751799827,"user_tz":-180,"elapsed":593,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}},"outputId":"9093a2a9-b387-4ab6-e35f-0b456fc6a98c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef583bc15f034db18e1c526ee03fda4f"}},"metadata":{}}],"source":["Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"]},{"cell_type":"markdown","metadata":{"id":"yVeEhUCrc30L"},"source":["## Let's see what our Q-Learning table looks like now 👀"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"nmfchsTITw4q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712751803496,"user_tz":-180,"elapsed":226,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}},"outputId":"e7c088f2-6b2d-44e3-ce51-36dd1069f41c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n","       [0.73509189, 0.        , 0.81450625, 0.77378094],\n","       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n","       [0.81450625, 0.        , 0.77378094, 0.77378094],\n","       [0.77378094, 0.81450625, 0.        , 0.73509189],\n","       [0.        , 0.        , 0.        , 0.        ],\n","       [0.        , 0.9025    , 0.        , 0.81450625],\n","       [0.        , 0.        , 0.        , 0.        ],\n","       [0.81450625, 0.        , 0.857375  , 0.77378094],\n","       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n","       [0.857375  , 0.95      , 0.        , 0.857375  ],\n","       [0.        , 0.        , 0.        , 0.        ],\n","       [0.        , 0.        , 0.        , 0.        ],\n","       [0.        , 0.9025    , 0.95      , 0.857375  ],\n","       [0.9025    , 0.95      , 1.        , 0.9025    ],\n","       [0.        , 0.        , 0.        , 0.        ]])"]},"metadata":{},"execution_count":20}],"source":["Qtable_frozenlake"]},{"cell_type":"markdown","metadata":{"id":"pUrWkxsHccXD"},"source":["## The evaluation method 📝\n","\n","- We defined the evaluation method that we're going to use to test our Q-Learning agent."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"jNl0_JO2cbkm","executionInfo":{"status":"ok","timestamp":1712751812248,"user_tz":-180,"elapsed":3,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}}},"outputs":[],"source":["def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n","  \"\"\"\n","  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n","  :param env: The evaluation environment\n","  :param max_steps: Maximum number of steps per episode\n","  :param n_eval_episodes: Number of episode to evaluate the agent\n","  :param Q: The Q-table\n","  :param seed: The evaluation seed array (for taxi-v3)\n","  \"\"\"\n","  episode_rewards = []\n","  for episode in tqdm(range(n_eval_episodes)):\n","    if seed:\n","      state, info = env.reset(seed=seed[episode])\n","    else:\n","      state, info = env.reset()\n","    step = 0\n","    truncated = False\n","    terminated = False\n","    total_rewards_ep = 0\n","\n","    for step in range(max_steps):\n","      # Take the action (index) that have the maximum expected future reward given that state\n","      action = greedy_policy(Q, state)\n","      new_state, reward, terminated, truncated, info = env.step(action)\n","      total_rewards_ep += reward\n","\n","      if terminated or truncated:\n","        break\n","      state = new_state\n","    episode_rewards.append(total_rewards_ep)\n","  mean_reward = np.mean(episode_rewards)\n","  std_reward = np.std(episode_rewards)\n","\n","  return mean_reward, std_reward"]},{"cell_type":"markdown","metadata":{"id":"0jJqjaoAnxUo"},"source":["## Evaluate our Q-Learning agent 📈\n","\n","- Usually, you should have a mean reward of 1.0\n","- The **environment is relatively easy** since the state space is really small (16). What you can try to do is [to replace it with the slippery version](https://gymnasium.farama.org/environments/toy_text/frozen_lake/), which introduces stochasticity, making the environment more complex."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"fAgB7s0HEFMm","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["95863a237e7b4c0886162c2732cf9876","c45cb308fd634cf88b53fd4641bf04c2","bf834c96678044b2afadafd23cee671b","7ae8b8a217e74c7db52117ea21893dcb","3a52fc03c7fb441f9798859fad288482","a53dcd1b7c5f45aaa3ea532e21468adb","15ec85ac884543a7bc14c63f11617648","22d63cceaef54d288f154f5855f45926","45fa0a37146a43d49e830b3b5928a9fc","ac212d91cc5a49d2a64dc454bd8a92c6","bbdd3f2c99364f6192cc134661fbb35d"]},"executionInfo":{"status":"ok","timestamp":1712751816034,"user_tz":-180,"elapsed":244,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}},"outputId":"2495eabd-9238-423e-da71-502f66e5f6be"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/100 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95863a237e7b4c0886162c2732cf9876"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Mean_reward=1.00 +/- 0.00\n"]}],"source":["# Evaluate our Agent\n","mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n","print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"QZ5LrR-joIHD"},"source":["#### Do not modify this code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jex3i9lZ8ksX"},"outputs":[],"source":["from huggingface_hub import HfApi, snapshot_download\n","from huggingface_hub.repocard import metadata_eval_result, metadata_save\n","\n","from pathlib import Path\n","import datetime\n","import json"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"Qo57HBn3W74O","executionInfo":{"status":"ok","timestamp":1712751840143,"user_tz":-180,"elapsed":241,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}}},"outputs":[],"source":["def record_video(env, Qtable, out_directory, fps=1):\n","  \"\"\"\n","  Generate a replay video of the agent\n","  :param env\n","  :param Qtable: Qtable of our agent\n","  :param out_directory\n","  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n","  \"\"\"\n","  images = []\n","  terminated = False\n","  truncated = False\n","  state, info = env.reset(seed=random.randint(0,500))\n","  img = env.render()\n","  images.append(img)\n","  while not terminated or truncated:\n","    # Take the action (index) that have the maximum expected future reward given that state\n","    action = np.argmax(Qtable[state][:])\n","    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n","    img = env.render()\n","    images.append(img)\n","  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=1)"]},{"cell_type":"code","source":["  # Step 6: Record a video\n","  record_video(env, Qtable_frozenlake, \"replay.mp4\")\n"],"metadata":{"id":"GAb247_aB3iz","executionInfo":{"status":"ok","timestamp":1712751947757,"user_tz":-180,"elapsed":233,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["from ipywidgets import Video\n","video = Video.from_file('replay.mp4')\n","video\n"],"metadata":{"id":"YrkmHYN4HTLk","colab":{"base_uri":"https://localhost:8080/","height":282,"referenced_widgets":["5701409e100a4137a24013a9a0eee822","d1a97ecac8774a419e9ac48d1b08bc46"]},"executionInfo":{"status":"ok","timestamp":1712751950081,"user_tz":-180,"elapsed":368,"user":{"displayName":"Sukosd Endre","userId":"02963673169135048018"}},"outputId":"45e20963-d367-44d3-c51d-cdf59bf38d6f"},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":["Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free\\x00\\x00v\\x1bmdat\\x00\\x…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5701409e100a4137a24013a9a0eee822"}},"metadata":{}}]},{"cell_type":"markdown","source":["# Experiment\n","- try changeing the hyperparameters - learning parameter, epsilon, steps, episodes, gamme discount rate\n","- try specifying a different map (impossible solution)\n","- try slippery parameter (`slippery=True`)"],"metadata":{"id":"5U0Qh4dF-SX_"}}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/huggingface/deep-rl-class/blob/master/notebooks/unit2/unit2.ipynb","timestamp":1712486038684}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ef583bc15f034db18e1c526ee03fda4f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fdf43db8095641c396e5716cbb85abc8","IPY_MODEL_20a90c62314e4a64b2aee2eebc11c0f8","IPY_MODEL_a8df2395c3f146a082b0972e9bd9f41d"],"layout":"IPY_MODEL_6bfccfb688604aba97116c9e8aa26427"}},"fdf43db8095641c396e5716cbb85abc8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c97fc82d4c954340ae3b4c4e0be28a29","placeholder":"​","style":"IPY_MODEL_5b52b4e6566245d58c280ac45a0c2053","value":"100%"}},"20a90c62314e4a64b2aee2eebc11c0f8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e549a0a77a5d4c809ef53e6a8d9ff896","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4a57ca21d1484c7aa907d0e756298a53","value":1000}},"a8df2395c3f146a082b0972e9bd9f41d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6f2e991c84c476bbc6543e78b6ddaf0","placeholder":"​","style":"IPY_MODEL_6a47fececcbc4d7ca39fdc7d7a4c5ee6","value":" 1000/1000 [00:00&lt;00:00, 3159.76it/s]"}},"6bfccfb688604aba97116c9e8aa26427":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c97fc82d4c954340ae3b4c4e0be28a29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b52b4e6566245d58c280ac45a0c2053":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e549a0a77a5d4c809ef53e6a8d9ff896":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a57ca21d1484c7aa907d0e756298a53":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c6f2e991c84c476bbc6543e78b6ddaf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a47fececcbc4d7ca39fdc7d7a4c5ee6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95863a237e7b4c0886162c2732cf9876":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c45cb308fd634cf88b53fd4641bf04c2","IPY_MODEL_bf834c96678044b2afadafd23cee671b","IPY_MODEL_7ae8b8a217e74c7db52117ea21893dcb"],"layout":"IPY_MODEL_3a52fc03c7fb441f9798859fad288482"}},"c45cb308fd634cf88b53fd4641bf04c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a53dcd1b7c5f45aaa3ea532e21468adb","placeholder":"​","style":"IPY_MODEL_15ec85ac884543a7bc14c63f11617648","value":"100%"}},"bf834c96678044b2afadafd23cee671b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_22d63cceaef54d288f154f5855f45926","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_45fa0a37146a43d49e830b3b5928a9fc","value":100}},"7ae8b8a217e74c7db52117ea21893dcb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac212d91cc5a49d2a64dc454bd8a92c6","placeholder":"​","style":"IPY_MODEL_bbdd3f2c99364f6192cc134661fbb35d","value":" 100/100 [00:00&lt;00:00, 2165.01it/s]"}},"3a52fc03c7fb441f9798859fad288482":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a53dcd1b7c5f45aaa3ea532e21468adb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15ec85ac884543a7bc14c63f11617648":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22d63cceaef54d288f154f5855f45926":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45fa0a37146a43d49e830b3b5928a9fc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac212d91cc5a49d2a64dc454bd8a92c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbdd3f2c99364f6192cc134661fbb35d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5701409e100a4137a24013a9a0eee822":{"model_module":"@jupyter-widgets/controls","model_name":"VideoModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VideoModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VideoView","autoplay":true,"controls":true,"format":"mp4","height":"","layout":"IPY_MODEL_d1a97ecac8774a419e9ac48d1b08bc46","loop":true,"width":""}},"d1a97ecac8774a419e9ac48d1b08bc46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}